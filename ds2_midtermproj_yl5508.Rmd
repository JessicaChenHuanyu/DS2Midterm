---
title: "ds2_miterm_yl5508"
author: "Yifei Liu"
date: 2023/03/22
output: html_document
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(ggridges)
library(corrplot)
library(ggcorrplot)
library(pheatmap)
library(rsample)
library(lattice)
library(caret)
library(pls)
library(rpart)
library(rpart.plot)
```

## Data Wrangling

```{r}
load('~/data science II/midterm/recovery.RData')

covid = as_tibble(dat) |>
  na.omit() |>
  janitor::clean_names() |>
  mutate(gender = factor(gender), 
         hypertension = factor(hypertension), 
         diabetes = factor(diabetes), 
         vaccine = factor(vaccine), 
         severity = factor(severity), 
         race = factor(race), 
         smoking = factor(smoking)) |>
  select(- id) |>
  relocate(recovery_time)

set.seed(11)
covid_split = initial_split(covid, prop = 0.8)
training = training(covid_split)
testing = testing(covid_split)
xtrain = model.matrix(recovery_time ~ ., training)[,-1]
ytrain = training$recovery_time
xtest = model.matrix(recovery_time ~ ., testing)[,-1]
ytest = testing$recovery_time

# showing connection between the response and other variables
theme1 = trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(.8, .1, .1, 1)
theme1$plot.line$lwd = 2
theme1$strip.background$col = rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

par(mar = c(4, 2, 1, 1), mfrow = c(4, 4))
x = model.matrix(recovery_time ~ age + bmi + weight + height + ldl + sbp, covid)[,-1]
y = covid$recovery_time
caret::featurePlot(x, y, plot = "scatter", labels = c("", "Recovery Time"), type = c("p"), layout = c(3, 2), main = "Continuous Variables Feature Plots")
```

## Boxplot of Recovery Time by Smoking Status and Gender

```{r}
# Boxplot
covid |>
  mutate(race = factor(race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic")), 
         smoking = factor(smoking, levels = c(0, 1, 2), labels = c("Never smoked", "Former smoker", "Current smoker"))) |>
  ggplot(aes(x = race, y = recovery_time, fill = race)) +
  geom_boxplot(alpha = 0.5) +
  labs(
    title = "Recovery Time by Race and Smoking Status",
    x = "Smoking",
    y = "Recovery Time"
  ) +
  guides(fill = guide_legend("Race")) +
  theme_minimal() +
  facet_grid(~ smoking) +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

```{r}
# Ridge plots
covid |>
  mutate(race = factor(race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic")), 
         smoking = factor(smoking, levels = c(0, 1, 2), labels = c("Never smoked", "Former smoker", "Current smoker"))) |>
  ggplot(aes(y = race, x = recovery_time, fill = race)) +
  geom_density_ridges(alpha = 0.5) +
  labs(
    title = "Recovery Time by Race and Smoking Status",
    x = "Smoking",
    y = "Recovery Time"
  ) +
  guides(fill = guide_legend("Race")) +
  theme_minimal() +
  facet_grid(~ smoking) +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

```{r}
# Boxplot
covid |>
  mutate(vaccine = factor(vaccine, levels = c(0, 1), labels = c("Not vaccinated", "Vaccinated")), 
         severity = factor(severity, levels = c(0, 1), labels = c("Not severe", "Severe"))) |>
  ggplot(aes(x = severity, y = recovery_time, fill = severity)) +
  geom_boxplot(alpha = 0.5) +
  labs(
    title = "Recovery Time by Vaccine and Severity",
    x = "Vaccine",
    y = "Recovery Time"
  ) +
  guides(fill = guide_legend("Severity")) +
  theme_minimal() +
  facet_grid(~ vaccine) +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

```{r}
# Ridge plots
covid |>
  mutate(vaccine = factor(vaccine, levels = c(0, 1), labels = c("Not vaccinated", "Vaccinated")), 
         severity = factor(severity, levels = c(0, 1), labels = c("Not severe", "Severe"))) |>
  ggplot(aes(y = severity, x = recovery_time, fill = severity)) +
  geom_density_ridges(alpha = 0.5) +
  labs(
    title = "Recovery Time by Vaccine and Severity",
    x = "Vaccine",
    y = "Recovery Time"
  ) +
  guides(fill = guide_legend("Severity")) +
  theme_minimal() +
  facet_grid(~ vaccine) +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

```{r}
covid |>
  mutate(gender = factor(gender, levels = c(0, 1), labels = c("Female", "Male")), 
         smoking = factor(smoking, levels = c(0, 1, 2), labels = c("Never smoked", "Former smoker", "Current smoker"))) |>
  ggplot(aes(x = recovery_time, fill = gender)) +
  geom_boxplot() +
  labs(title = "Boxplot of Recovery Time by Smoking Status and Gender",
       x = "Recovery Time",
       y = "Smoking Status",
       fill = "Gender") +
  theme_bw() +
  facet_wrap(~ smoking, ncol = 1) + 
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

## Correlation Plot

```{r}
par(mar = c(1, 1, 1, 1), mfrow=c(1,1))
corrplot::corrplot(cor(covid |> select(recovery_time, age, bmi, weight, height, ldl, sbp)), type = "upper", order = "AOE", diag = FALSE, title = "Correlation Plot of Variables", cex.main = 1.5, mar = c(0, 0, 1, 0))

covid_numeric = 
  covid |>
  select(where(is.numeric))

ggcorrplot(cor(covid_numeric), lab = T) +
  ggtitle("Correlation Plot of Continuous Variables") + 
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

## Lasso

Model assumptions:\
(a) Sparsity Assumption: Lasso assumes that the true model depends on only a small number of predictors, implying that the model is sparse.  This means it's suited for scenarios where only a few variables significantly impact the response variable.\
(b) Regularization: By penalizing the magnitude of the coefficients (L1 penalty), Lasso encourages smaller absolute values of coefficients, thus reducing model complexity and the risk of overfitting.\

```{r warning = FALSE, message=FALSE}
set.seed(11)

ctrl = trainControl(method = 'cv', number = 10)
ctrl_1se = trainControl(method = 'cv', number = 10, selectionFunction =  'oneSE')

lasso.fit = train(recovery_time ~ ., data = training, 
             method = 'glmnet', 
             tuneGrid = expand.grid(alpha = 1, 
                                    lambda = exp(seq(-4, -2, length = 100))), 
             trControl = ctrl)

plot(lasso.fit, xTrans = log, main = "Lasso CV Result")

# selected lambda
lasso.fit$bestTune$lambda

# coefficients
coef(lasso.fit$finalModel, s = lasso.fit$bestTune$lambda)

# num of predictors
sum(lasso.fit$coefname != 0)
```

```{r warning = FALSE, message=FALSE}
# applying 1se rule
lasso.fit.1se = train(recovery_time ~ ., data = training, 
             method = 'glmnet', 
             tuneGrid = expand.grid(alpha = 1, 
                                    lambda = exp(seq(-4, -2, length = 100))), 
             trControl = ctrl_1se)

plot(lasso.fit.1se, xTrans = log, main = "Lasso_1se CV Result")

# selected alpha and lambda
lasso.fit.1se$bestTune$lambda

# coefficients
coef(lasso.fit.1se$finalModel, s = lasso.fit.1se$bestTune$lambda)

# num of predictors
sum(lasso.fit.1se$coefname != 0)
```

## Elastic Net

Model assumptions:\
(a) Combined Regularization: Elastic Net uses both L1 and L2 regularization, combining Lasso's variable selection capability with Ridge regression's ability to handle highly correlated predictors.\
(b) Adjusting Regularization Balance: Elastic Net has two regularization parameters, controlling the overall strength of regularization and the weight balance between L1 and L2 terms. This offers more flexible model tuning capability.\

```{r}
set.seed(11)
ctrl = trainControl(method = 'cv', number = 10)
ctrl_1se = trainControl(method = 'cv', number = 10, selectionFunction =  'oneSE')

enet.fit = train(recovery_time ~ ., data = training, 
             method = 'glmnet', 
             tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                    lambda = exp(seq(-6, 1, length = 100))), 
             trControl = ctrl)

myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol), superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar, xTrans = log, main = "Elastic Net CV Result")

# selected alpha and lambda
enet.fit$bestTune

# coefficients
coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)

# num of predictors
sum(enet.fit$coefname != 0)
```


```{r}
# applying 1se rule
set.seed(11)
enet.fit.1se = train(recovery_time ~ ., data = training, 
             method = 'glmnet', 
             tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                    lambda = exp(seq(-6, 1, length = 100))), 
             trControl = ctrl_1se)

plot(enet.fit.1se, par.settings = myPar, xTrans = log, main = "Elastic Net_1se CV Result")

# selected alpha and lambda
enet.fit.1se$bestTune
```

## PLS

Model assumptions:\
(a) Linear Relationship: PLS assumes a linear relationship between the independent variables and the response variable. It aims to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space.\
(b) PLS assumes that the structure of the relationship between X and Y variables can be captured through a few latent structures. This is fundamental to reducing dimension and extracting the most relevant information from X that predicts Y.\

```{r}
set.seed(11)

#pls = plsr(recovery_time ~ ., data = training, scale = TRUE, validation = 'CV')
#summary(pls)
#validationplot(pls, val.type = 'MSEP', legendpos = 'topright')
#cv.mse = RMSEP(pls)
#ncomp.cv = which.min(cv.mse$val[1,,]) - 1

pls.fit <- train(recovery_time ~ ., data = training, 
                 method = "pls", 
                 tuneGrid = data.frame(ncomp = 1:15), 
                 trControl = ctrl, 
                 preProcess = c("center", "scale"))

ggplot(pls.fit, highlight = TRUE) +
  theme_bw() +
  labs(title = "PLS CV Result") +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))

summary(pls.fit)
pls.fit$bestTune
```

## MARS

Model assumptions:\
(a) Non-linearity and Interaction: MARS does not assume that relationships between the independent variables and the dependent variable are linear or follow a specific functional form. Instead, it adaptively fits piecewise linear regressions that can model complex non-linear relationships and interactions among variables.\
(b) Distribution of Errors: MARS does not make specific assumptions about the distribution of error terms.\

```{r message = FALSE, warning = FALSE}
set.seed(11)
mars_grid = expand.grid(degree = 1:6, nprune = 2:15)
ctrl = trainControl(method = 'cv', number = 10)

mars.fit = train(xtrain, ytrain, 
                 method = "earth", 
                 tuneGrid = mars_grid, 
                 trControl = ctrl)

ggplot(mars.fit) +
  theme_bw() +
  labs(title = "MARS CV Result") +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))

# fit of the model
mars.fit$bestTune
coef(mars.fit$finalModel)

# partial dependence plot (PDP)
p1 = pdp::partial(mars.fit, pred.var = c("bmi"), grid.resolution = 10) |> autoplot()

p2 = pdp::partial(mars.fit, pred.var = c("bmi", "race2"),
grid.resolution = 10) |>
pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE,
screen = list(z = 20, x = -60))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

## GAM

Model assumptions:\
(a) Additivity: The effect of each predictor on the response is additive. The total effect on the response variable is the sum of the effects of each predictor, modeled by its own smooth function.\
(b) Smoothness of the Predictor Functions: The relationships between the predictors and the response can be adequately modeled using smooth functions. The degree of smoothness is usually determined by the data and is controlled by smoothing parameters, which can be estimated from the data itself.\

```{r}
set.seed(11)
gam.fit = train(xtrain, ytrain, 
                method = "gam", 
                trControl = ctrl)
gam.fit$finalModel

# plot (gam.fit)
par(oma = c(0, 0, 3, 0))
par(mar = c(4, 4, 1, 1), mfrow = c(2, 3))
plot(gam.fit$finalModel)
mtext("GAM Result", side = 3, line = 0.5, outer = TRUE, cex = 1.2)
```

## Random Forest

Model assumptions:\

```{r}
set.seed(11)
rf.fit.1 = rpart(recovery_time ~ ., data = training, 
               control = rpart.control(cp = 0.003))
rpart.plot(rf.fit.1)
printcp(rf.fit.1)
plotcp(rf.fit.1)

cptable = rf.fit.1$cptable
minErr = which.min(cptable[,4])
rf.fit.final = rpart::prune(rf.fit.1, cp = cptable[minErr,1])
rpart.plot(rf.fit.final)
plotcp(rf.fit.final)

# test error
rf.test = predict(rf.fit.final, x = xtest)
rf.rmse = sqrt(mean((rf.test - ytest)^2))
rf.rmse

rf.r2.res = sum((ytest - rf.test)^2)
rf.r2.tot = sum((ytest - mean(ytest))^2)
rf.r2 = 1 - (rf.r2.res/rf.r2.tot)

rf.mae = mean(abs(rf.test - ytest))
```

```{r}
set.seed(11)
rf.fit = train(xtrain, ytrain, 
               method = "rf", 
               trControl = ctrl, 
               tuneGrid = expand.grid(mtry = 1:10))

ctree.fit = train(recovery_time ~ ., training, 
                  method = "ctree", 
                  tuneGrid = data.frame(mincriterion = 1-exp(seq(-6, -2, length = 100))), 
                  trControl = ctrl)

# plot
ggplot(rf.fit, highlight = TRUE) + 
  theme_bw() +
  labs(title = "Random Forest (rf.fit) CV Result") +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))

ggplot(ctree.fit, highlight = TRUE) + 
  theme_bw() +
  labs(title = "Random Forest (ctree.fit) CV Result") +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))

plot(ctree.fit$finalModel)
```

## Model Comparation

```{r}
library(patchwork)
res = resamples(list(lasso = lasso.fit, 
                     lasso_1se = lasso.fit.1se, 
                     enet = enet.fit, 
                     enet_1se = enet.fit.1se, 
                     pls = pls.fit, 
                     mars = mars.fit, 
                     gam = gam.fit, 
                     rf = rf.fit, 
                     rf_ctree = ctree.fit))$value |>
  tibble() |>
  janitor::clean_names() |>
  select(- resample) |>
  pivot_longer(
    everything(),
    names_to = c(".value", "metric"),
    names_pattern = "(.*)_(.*)"
  ) |>
  pivot_longer(c(2:10), names_to = "model", values_to = "result")

plot_rmse = res |>
  filter(metric == "rmse") |>
  ggplot(aes(x = model, y = result, fill = model)) +
  geom_boxplot(alpha = 0.5) +
  labs(y = "RMSE") +
  theme_minimal() +
  guides(fill = guide_legend("Model"))

plot_r_squared = res |>
  filter(metric == "rsquared") |>
  ggplot(aes(x = model, y = result, fill = model)) +
  geom_boxplot(alpha = 0.5) +
  labs(y = "R squared") +
  theme_minimal() +
  guides(fill = guide_legend("Model"))

plot_mae = res |>
  filter(metric == "mae") |>
  ggplot(aes(x = model, y = result, fill = model)) +
  geom_boxplot(alpha = 0.5) +
  labs(y = "MAE") +
  theme_minimal() +
  guides(fill = guide_legend("Model"))

final_plot = plot_rmse + plot_r_squared + plot_mae +
  plot_layout(ncol = 3) + 
  plot_annotation(title = "Performance by Models and Metrics",
                  theme = theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5)))
```

```{r fig.width=15, fig.height=6}
final_plot
```

```{r}
# testing data
pred.lasso = predict(lasso.fit, newdata = testing)
rmse.lasso = sqrt(mean((testing$recovery_time - pred.lasso)^2))
r2.lasso = cor(testing$recovery_time, pred.lasso)^2
mae.lasso = mean(abs(testing$recovery_time - pred.lasso))

pred.lasso.1se = predict(lasso.fit.1se, newdata = testing)
rmse.lasso_1se = sqrt(mean((testing$recovery_time - pred.lasso.1se)^2))
r2.lasso_1se = cor(testing$recovery_time, pred.lasso.1se)^2
mae.lasso_1se = mean(abs(testing$recovery_time - pred.lasso.1se))

pred.enet = predict(enet.fit, newdata = testing)
rmse.enet = sqrt(mean((testing$recovery_time - pred.enet)^2))
r2.enet = cor(testing$recovery_time, pred.enet)^2
mae.enet = mean(abs(testing$recovery_time - pred.enet))

pred.enet.1se = predict(enet.fit.1se, newdata = testing)
rmse.enet_1se = sqrt(mean((testing$recovery_time - pred.enet.1se)^2))
r2.enet_1se = cor(testing$recovery_time, pred.enet.1se)^2
mae.enet_1se = mean(abs(testing$recovery_time - pred.enet.1se))

pred.pls = predict(pls.fit, newdata = testing)
rmse.pls = sqrt(mean((testing$recovery_time - pred.pls)^2))
r2.pls = cor(testing$recovery_time, pred.pls)^2
mae.pls = mean(abs(testing$recovery_time - pred.pls))

pred.mars = predict(mars.fit, newdata = xtest)
rmse.mars = sqrt(mean((testing$recovery_time - pred.mars)^2))
r2.mars = cor(testing$recovery_time, pred.mars)^2 |> as.numeric()
mae.mars = mean(abs(testing$recovery_time - pred.mars))

pred.gam = predict(gam.fit, newdata = xtest)
rmse.gam = sqrt(mean((testing$recovery_time - pred.gam)^2))
r2.gam = cor(testing$recovery_time, pred.gam)^2
mae.gam = mean(abs(testing$recovery_time - pred.gam))

pred.rf = predict(rf.fit, newdata = xtest)
rmse.rf = sqrt(mean((testing$recovery_time - pred.rf)^2))
r2.rf = cor(testing$recovery_time, pred.rf)^2
mae.rf = mean(abs(testing$recovery_time - pred.rf))

pred.ctree = predict(ctree.fit, newdata = testing)
rmse.rf_ctree = sqrt(mean((testing$recovery_time - pred.ctree)^2))
r2.rf_ctree = cor(testing$recovery_time, pred.ctree)^2
mae.rf_ctree = mean(abs(testing$recovery_time - pred.ctree))

res_test = tibble(
  rmse.lasso, r2.lasso, mae.lasso, 
  rmse.lasso_1se, r2.lasso_1se, mae.lasso_1se, 
  rmse.enet, r2.enet, mae.enet, 
  rmse.enet_1se, r2.enet_1se, mae.enet_1se, 
  rmse.pls, r2.pls, mae.pls, 
  rmse.mars, r2.mars, mae.mars, 
  rmse.gam, r2.gam, mae.gam, 
  rmse.rf, r2.rf, mae.rf, 
  rmse.rf_ctree, r2.rf_ctree, mae.rf_ctree
) |>
  pivot_longer(
    everything(),
    names_to = c("metric", "model"),
    names_sep = "\\.",
    values_to = "result"
  )
```

```{r fig.width=8, fig.height=12}
res_test |>
  ggplot(aes(x = model, y = result, group = metric, color = metric)) +
  geom_line(size = 0.8) +
  labs(
    title = "Testing Performance by Models and Metrics",
    x = "Model",
    y = "Performance"
  ) +
  guides(color = guide_legend("Metric")) +
  facet_wrap(~ metric, ncol = 1, scales = "free_y") + 
  theme_bw() +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

