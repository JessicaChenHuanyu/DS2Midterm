---
title: "ds2_miterm_yl5508"
author: "Yifei Liu"
date: 2023/03/22
output: html_document
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(ggridges)
library(corrplot)
library(rsample)
library(lattice)
library(caret)
library(pls)
```

## Data Wrangling

```{r}
load('~/data science II/midterm/recovery.RData')

covid = as_tibble(dat) |>
  na.omit() |>
  janitor::clean_names() |>
  mutate(gender = factor(gender), 
         hypertension = factor(hypertension), 
         diabetes = factor(diabetes), 
         vaccine = factor(vaccine, levels = c(0, 1), labels = c("Not vaccinated", "Vaccinated")), 
         severity = factor(severity, levels = c(0, 1), labels = c("Not severe", "Severe")), 
         race = factor(race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic")), 
         smoking = factor(smoking, levels = c(0, 1, 2), labels = c("Never smoked", "Former smoker", "Current smoker"))) |>
  select(- id) |>
  relocate(recovery_time)

set.seed(11)
covid_split = initial_split(covid, prop = 0.8)
training = training(covid_split)
testing = testing(covid_split)
xtrain = model.matrix(recovery_time ~ ., training)[,-1]
ytrain = training$recovery_time
xtest = model.matrix(recovery_time ~ ., testing)[,-1]
ytest = testing$recovery_time

# showing connection between the response and other variables
theme1 = trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(.8, .1, .1, 1)
theme1$plot.line$lwd = 2
theme1$strip.background$col = rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

par(mar = c(4, 2, 1, 1), mfrow = c(4, 4))
x = model.matrix(recovery_time ~ age + bmi + weight + height + ldl + sbp, covid)[,-1]
y = covid$recovery_time
caret::featurePlot(x, y, plot = "scatter", labels = c("", "Y"), type = c("p"), layout = c(3, 2))
```

## Boxplot of Recovery Time by Smoking Status and Gender

```{r}
# Boxplot
covid |>
  ggplot(aes(x = race, y = recovery_time, fill = race)) +
  geom_boxplot(alpha = 0.5) +
  labs(
    title = "Recovery Time by Race and Smoking Status",
    x = "Smoking",
    y = "Recovery Time"
  ) +
  guides(fill = guide_legend("Race")) +
  theme_minimal() +
  facet_grid(~ smoking) +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

```{r}
# Ridge plots
covid |>
  ggplot(aes(y = race, x = recovery_time, fill = race)) +
  geom_density_ridges(alpha = 0.5) +
  labs(
    title = "Recovery Time by Race and Smoking Status",
    x = "Smoking",
    y = "Recovery Time"
  ) +
  guides(fill = guide_legend("Race")) +
  theme_minimal() +
  facet_grid(~ smoking) +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

```{r}
# Boxplot
covid |>
  ggplot(aes(x = severity, y = recovery_time, fill = severity)) +
  geom_boxplot(alpha = 0.5) +
  labs(
    title = "Recovery Time by Vaccine and Severity",
    x = "Vaccine",
    y = "Recovery Time"
  ) +
  guides(fill = guide_legend("Severity")) +
  theme_minimal() +
  facet_grid(~ vaccine) +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

```{r}
# Ridge plots
covid |>
  ggplot(aes(y = severity, x = recovery_time, fill = severity)) +
  geom_density_ridges(alpha = 0.5) +
  labs(
    title = "Recovery Time by Vaccine and Severity",
    x = "Vaccine",
    y = "Recovery Time"
  ) +
  guides(fill = guide_legend("Severity")) +
  theme_minimal() +
  facet_grid(~ vaccine) +
  theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
```

## Correlation Plot

```{r}
par(mar = c(1, 1, 1, 1), mfrow=c(1,1))
corrplot::corrplot(cor(covid |> select(recovery_time, age, bmi, weight, height, ldl, sbp)), type = "upper", order = "AOE", diag = FALSE, title = "Correlation Plot of Variables", cex.main = 1.5, mar = c(0, 0, 1, 0))
```

## Lasso

Model assumptions:\
(a) Sparsity Assumption: Lasso assumes that the true model depends on only a small number of predictors, implying that the model is sparse.  This means it's suited for scenarios where only a few variables significantly impact the response variable.\
(b) Regularization: By penalizing the magnitude of the coefficients (L1 penalty), Lasso encourages smaller absolute values of coefficients, thus reducing model complexity and the risk of overfitting.\

```{r warning = FALSE, message=FALSE}
set.seed(11)

ctrl = trainControl(method = 'cv', number = 10)
ctrl_1se = trainControl(method = 'cv', number = 10, selectionFunction =  'oneSE')

lasso.fit = train(recovery_time ~ ., data = training, 
             method = 'glmnet', 
             tuneGrid = expand.grid(alpha = 1, 
                                    lambda = exp(seq(-5, -2, length = 100))), 
             trControl = ctrl)

plot(lasso.fit, xTrans = log)

# selected lambda
lasso.fit$bestTune$lambda

# coefficients
coef(lasso.fit$finalModel, s = lasso.fit$bestTune$lambda)

# num of predictors
sum(lasso.fit$coefname != 0)

# test error (RMSE)
pred.lasso = predict(lasso.fit, newdata = testing)
rmse.lasso = sqrt(mean((testing$recovery_time - pred.lasso)^2))
rmse.lasso
```

```{r warning = FALSE, message=FALSE}
# applying 1se rule
lasso.fit.1se = train(recovery_time ~ ., data = training, 
             method = 'glmnet', 
             tuneGrid = expand.grid(alpha = 1, 
                                    lambda = exp(seq(-5, -2, length = 100))), 
             trControl = ctrl_1se)

plot(lasso.fit.1se, xTrans = log)

# selected alpha and lambda
lasso.fit.1se$bestTune$lambda

# coefficients
coef(lasso.fit.1se$finalModel, s = lasso.fit.1se$bestTune$lambda)

# num of predictors
sum(lasso.fit.1se$coefname != 0)

# test error (RMSE)
pred.lasso.1se = predict(lasso.fit.1se, newdata = testing)
rmse.lasso.1se = sqrt(mean((testing$recovery_time - pred.lasso.1se)^2))
rmse.lasso.1se
```

## Elastic Net

Model assumptions:\
(a) Combined Regularization: Elastic Net uses both L1 and L2 regularization, combining Lasso's variable selection capability with Ridge regression's ability to handle highly correlated predictors.\
(b) Adjusting Regularization Balance: Elastic Net has two regularization parameters, controlling the overall strength of regularization and the weight balance between L1 and L2 terms. This offers more flexible model tuning capability.\

```{r}
set.seed(11)
ctrl = trainControl(method = 'cv', number = 10)
ctrl_1se = trainControl(method = 'cv', number = 10, selectionFunction =  'oneSE')

enet.fit = train(recovery_time ~ ., data = training, 
             method = 'glmnet', 
             tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                    lambda = exp(seq(-8, 0, length = 100))), 
             trControl = ctrl)

myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol), superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar)

# selected alpha and lambda
enet.fit$bestTune

# coefficients
coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)

# num of predictors
sum(enet.fit$coefname != 0)

# test error (RMSE)
pred.enet = predict(enet.fit, newdata = testing)
rmse.enet = sqrt(mean((testing$recovery_time - pred.enet)^2))
rmse.enet
```


```{r}
# applying 1se rule
set.seed(11)
enet.fit.1se = train(recovery_time ~ ., data = training, 
             method = 'glmnet', 
             tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                    lambda = exp(seq(-8, 0, length = 100))), 
             trControl = ctrl_1se)

plot(enet.fit.1se, par.settings = myPar)

# selected alpha and lambda
enet.fit.1se$bestTune

# test error (RMSE)
pred.enet.1se = predict(enet.fit.1se, newdata = testing)
rmse.enet.1se = sqrt(mean((testing$recovery_time - pred.enet.1se)^2))
rmse.enet.1se
```

## PLS

Model assumptions:\
(a) Linear Relationship: PLS assumes a linear relationship between the independent variables and the response variable. It aims to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space.\
(b) PLS assumes that the structure of the relationship between X and Y variables can be captured through a few latent structures. This is fundamental to reducing dimension and extracting the most relevant information from X that predicts Y.\

```{r}
set.seed(11)
pls = plsr(recovery_time ~ ., data = training, scale = TRUE, validation = 'CV')
summary(pls)

validationplot(pls, val.type = 'MSEP', legendpos = 'topright')

cv.mse = RMSEP(pls)

# num of components
ncomp.cv = which.min(cv.mse$val[1,,]) - 1

# test error (RMSE)
pred.pls = predict(pls, newdata = testing, ncomp = ncomp.cv)
pls.rmse = sqrt(mean((testing$recovery_time - pred.pls)^2))
pls.rmse
```

## MARS

Model assumptions:\
(a) Non-linearity and Interaction: MARS does not assume that relationships between the independent variables and the dependent variable are linear or follow a specific functional form. Instead, it adaptively fits piecewise linear regressions that can model complex non-linear relationships and interactions among variables.\
(b) Distribution of Errors: MARS does not make specific assumptions about the distribution of error terms.\

```{r message = FALSE, warning = FALSE}
set.seed(11)
mars_grid = expand.grid(degree = 1:6, nprune = 2:15)
ctrl = trainControl(method = 'cv', number = 10)

mars.fit = train(xtrain, ytrain, 
                 method = "earth", 
                 tuneGrid = mars_grid, 
                 trControl = ctrl)

ggplot(mars.fit) +
  theme_bw()

# fit of the model
mars.fit$bestTune
coef(mars.fit$finalModel)

# partial dependence plot (PDP)
p1 = pdp::partial(mars.fit, pred.var = c("bmi"), grid.resolution = 10) %>% autoplot()

p2 = pdp::partial(mars.fit, pred.var = c("bmi", "race2"),
grid.resolution = 10) %>%
pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE,
screen = list(z = 20, x = -60))

gridExtra::grid.arrange(p1, p2, ncol = 2)

# test error (rmse)
mars.test = predict(mars.fit, x = xtest)
mars.rmse = sqrt(mean((mars.test - ytest)^2))
mars.rmse
```

## GAM

Model assumptions:\
(a) Additivity: The effect of each predictor on the response is additive. The total effect on the response variable is the sum of the effects of each predictor, modeled by its own smooth function.\
(b) Smoothness of the Predictor Functions: The relationships between the predictors and the response can be adequately modeled using smooth functions. The degree of smoothness is usually determined by the data and is controlled by smoothing parameters, which can be estimated from the data itself.\

```{r}
set.seed(11)
gam.fit = train(xtrain, ytrain, 
                method = "gam", 
                trControl = ctrl)
gam.fit$finalModel

# plot (gam.fit)
par(mar = c(4, 2, 1, 1), mfrow = c(2, 3))
plot(gam.fit$finalModel)

# test error (rmse)
gam.test = predict(gam.fit, x = xtest)
gam.rmse = sqrt(mean((gam.test - ytest)^2))
gam.rmse
```

## Model Comparation

```{r}
bwplot(resamples(list(lasso = lasso.fit, 
                      lasso_1se = lasso.fit.1se, 
                      enet = enet.fit, 
                      enet_1se = enet.fit.1se, 
                      mars = mars.fit, 
                      gam = gam.fit)), 
       metric = "RMSE")
```


