---
title: "Data Science II Midterm Project"
author: "Huanyu Chen"
output: pdf_document
---

```{r message=FALSE, warning = FALSE}
library(ggplot2)
library(tidyverse)
library(corrplot)

load("recovery.Rdata")
```

# Exploratory Analysis and Data Visualization

## Exploratory Analysis
In this dataset, `age`, `height`, `weight`, `bmi`, `SBP`, `LDL`, and `recovery_time` are continuous variables.
```{r}
continuous_vars <- dat[, c("age", "height", "weight", "bmi",
                           "SBP", "LDL", "recovery_time")]
summary(continuous_vars)
```

## Boxplot of Recovery Time by Smoking Status and Gender

Our analysis reveals a notable trend: across all smoking statuses, females (`gender` = 0) consistently exhibit longer recovery times compared to males. Interestingly, individuals who had never smoked had more outliers on the right side of the boxplot, suggesting a longer recovery time. This counter-intuitive finding suggests that individuals with healthier lifestyles, such as non-smokers, paradoxically require more time to recover from COVID-19.

```{r}
ggplot(dat, aes(x = recovery_time, fill = factor(gender))) +
  geom_boxplot() +
  labs(title = "Boxplot of Recovery Time by Smoking Status and Gender",
       x = "Recovery Time", y = "Smoking Status",
       fill = "Gender") +
  facet_wrap(~factor(smoking), ncol = 1) +
  theme_bw()
```

## Pairs

Our exploration of the variables age, BMI, and recovery time reveals no clear linear relationships among them. It implies that other complex factors beyond these variables might be influencing the recovery time from COVID-19, highlighting the complexity of analysis about recovery time.

```{r}
pairs(dat[, c("age", "bmi", "recovery_time")])
```

## Correlation Table

The correlation analysis conducted on variables including "height," "weight," and "bmi" suggests a strong positive correlation among these attributes, which aligns with our common understanding. However, no significant correlations were observed between these attributes and other variables in the dataset.

```{r}
correlation_matrix <- cor(dat[, c("age", "height", "weight", "bmi",
                                  "SBP", "LDL", "recovery_time")])
corrplot(correlation_matrix, method = "color")
```

\newpage
# Model Training
## Lasso
```{r warning = FALSE, message=FALSE}
library(caret)
library(glmnet)
library(pls)

set.seed(2024)
indexTrain <- createDataPartition(y = dat$recovery_time, p = 0.8, list = FALSE)
trainData <- dat[indexTrain, ]
testData <- dat[-indexTrain, ]

cv.lasso <- cv.glmnet(as.matrix(trainData[, -ncol(trainData)]),
                      trainData$recovery_time, 
                      alpha = 1, 
                      lambda = exp(seq(5, -5, length = 100)))
plot(cv.lasso)
selected_lambda <- cv.lasso$lambda.min
coefficients_min <- coef(cv.lasso, s = selected_lambda)
num_predictors_min <- sum(coefficients_min != 0)

test_predictions <- predict(cv.lasso, newx = as.matrix(testData[, -ncol(testData)]),
                            s = selected_lambda, type = "response")
test_error <- sqrt(mean((test_predictions - testData$recovery_time)^2))
print(test_error)
```

## PLS Model
```{r}
set.seed(2024)
pls_model <- plsr(recovery_time ~ ., data = trainData,
                  scale = TRUE, validation = "CV")

test_error_pls <- RMSEP(pls_model)
n_comp <- which.min(test_error_pls$val[1,,]) - 1

validationplot(pls_model, val.type = "MSEP", legendpos = "topright")
pred_pls_model <- predict(pls_model, newdata = testData, ncomp = n_comp)
test_error <- sqrt(mean((pred_pls_model - testData$recovery_time)^2))
print(test_error)
```

## DF: need to be done
```{r message = FALSE}
library(mgcv)
library(earth)
```


## MARS
```{r}
ctrl1 <- trainControl(method = "cv", number = 5)
set.seed(2024)
mars_grid <- expand.grid(degree = 1:4, nprune = 1:20)
mars.fit <- train(recovery_time ~ ., 
                  data = trainData, 
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)
ggplot(mars.fit)
```

```{r}
mars.fit$bestTune
coef(mars.fit$finalModel)
p1 = pdp::partial(mars.fit, pred.var = c("bmi", "age"), grid.resolution = 10) %>%
  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, screen = list(z = 40, x = -60))
p2 = pdp::partial(mars.fit, pred.var = c("bmi", "LDL"), grid.resolution = 10) %>%
  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, screen = list(z = 40, x = -60))
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

```{r}
mars_pred <- predict(mars.fit, newdata = testData)
y_test <- testData$recovery_time
squared_errors <- (mars_pred - y_test)^2
rmse <- sqrt(mean(squared_errors))
print(rmse)
```

## GAM
For the variables `height` and `bmi,` the residuals in the plots suggest that there appears to be some curvature or non-linearity in the relationship to `recovery_time`. Therefore, when modeling these variables, it may be necessary to consider more flexible approaches, such as including polynomial terms or using non-linear transformations to better capture the underlying relationship with the outcome variable.
```{r}
set.seed(2024)
gam.fit <- train(recovery_time ~ ., 
                 data = trainData, 
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp", select = TRUE),
                 trControl = ctrl1)
gam.fit$finalModel
```

```{r}
par(mar = c(1, 1, 1, 1), mfrow=c(4,4))
for (i in 1:length(gam.fit$finalModel$term.labels)) {
  plot(gam.fit$finalModel, residuals = TRUE, shade = TRUE,
       xlab = gam.fit$finalModel$term.labels[i], ylab = "Residuals")
  }
```

```{r}
gam_pred <- predict(gam.fit, newdata = testData)
y_test <- testData$recovery_time
squared_errors <- (gam_pred - y_test)^2
rmse <- sqrt(mean(squared_errors))
print(rmse)
```

\newpage
# Results: need to be done

The RMSE values obtained from Lasso and PLS models were comparable, suggesting that both models performed similarly in predicting the target variable `recovery_time`. This implies that both regularization techniques, despite their differences in approach, yielded comparable predictive performance in this scenario.

\newpage
# Conclusions