---
title: "P8106_midterm"
author: "lz2951"
date: "2024-03-28"
output: 
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{amssymb}
---

```{r}
library(tidyverse)
library(ggcorrplot)
library(pheatmap)
library(caret)
library(tidymodels)
```

# Import Data

```{r}
load("recovery.RData")

str(dat)

recovery = dat |>
  janitor::clean_names() |>
  mutate(gender = as.factor(gender), 
         hypertension = as.factor(hypertension),
         diabetes = as.factor(diabetes),
         vaccine = as.factor(vaccine),
         severity = as.factor(severity),
         study = as.factor(study)) |>
  select(-id)

str(recovery)
```

# Exploratory analysis and data visualization

```{r}
skimr::skim(recovery) |>
  select(-numeric.hist)
```

## Analysis between numeric predictors

```{r}
recovery_numeric = 
  recovery |>
  select(where(is.numeric))

# recovery_numeric

ggcorrplot(cor(recovery_numeric), lab = T)

recovery_numeric_long = 
  recovery_numeric |>
  gather(key = "predictor", value = "value", -recovery_time)

# recovery_numeric_long

ggplot(recovery_numeric_long, aes(x = value, y = recovery_time)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~predictor, scales = "free")
```

## Analysis between factor predictors

```{r}
recovery_factor = 
  recovery |>
  select(where(is.factor), recovery_time)

# recovery_factor

recovery_factor_nonresp =
  recovery |>
  select(where(is.factor))

# recovery_factor_nonresp

chi_sq_matrix = matrix(NA, ncol = ncol(recovery_factor_nonresp), nrow = ncol(recovery_factor_nonresp))
for (i in 1:(ncol(recovery_factor_nonresp)-1)) {
  for (j in (i+1):ncol(recovery_factor_nonresp)) {
    cross_table = table(recovery_factor_nonresp[,i],
                        recovery_factor_nonresp[,j])
    chi_sq_matrix[i,j] = chisq.test(cross_table)$p.value
  }
}

rownames(chi_sq_matrix) = colnames(recovery_factor_nonresp)
colnames(chi_sq_matrix) = colnames(recovery_factor_nonresp)

# chi_sq_matrix

chi_sq_matrix = t(chi_sq_matrix)

# chi_sq_matrix

pheatmap(chi_sq_matrix,
         cluster_rows = FALSE, cluster_cols = FALSE,
         show_rownames = TRUE, show_colnames = TRUE,
         legend = TRUE, display_numbers = TRUE)

recovery_factor_long = 
  recovery_factor |>
  gather(key = "predictor", value = "value", -recovery_time)

# recovery_factor_long

ggplot(recovery_factor_long, aes(x = value, y = recovery_time)) +
  geom_violin() +
  facet_wrap(~predictor, scales = "free")
```

## Analysis between numeric and factor predictors

```{r}
anova_matrix = matrix(NA, ncol = ncol(recovery_factor_nonresp), nrow = ncol(recovery_numeric))
for (i in 1:(ncol(recovery_numeric))) {
  for (j in 1:ncol(recovery_factor_nonresp)) {
    cross_dat = data.frame(num = recovery_numeric[,i], 
                           fac = recovery_factor_nonresp[,j])
    anova_matrix[i,j] = summary(aov(num ~ fac, data = cross_dat))[[1]]$"Pr(>F)"[[1]]
  }
}

# anova_matrix

rownames(anova_matrix) = colnames(recovery_numeric)
colnames(anova_matrix) = colnames(recovery_factor_nonresp)

pheatmap(anova_matrix,
         cluster_rows = FALSE, cluster_cols = FALSE,
         show_rownames = TRUE, show_colnames = TRUE,
         legend = TRUE, display_numbers = TRUE)
```

# Model training

Split dataset into training and testing data.

```{r}
set.seed(11)
data_split <- initial_split(recovery, prop = 0.8)

training_data <- training(data_split)
testing_data <- testing(data_split)
```

## Ridge regression

```{r}
ctrl1 <- trainControl(method = "cv", number = 10)

set.seed(11)
ridge.fit <- train(recovery_time ~ . ,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(6, -6, length = 200))),
                   trControl = ctrl1)

plot(ridge.fit, xTrans = log)

ridge.fit$bestTune

coef(ridge.fit$finalModel, ridge.fit$bestTune$lambda)
```

## Lasso

```{r}
set.seed(11)
lasso.fit <- train(recovery_time ~ .,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(6, -6, length = 200))),
                   trControl = ctrl1)
plot(lasso.fit, xTrans = log)

lasso.fit$bestTune

coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)
```

## Elastic net

```{r}
set.seed(11)
enet.fit <- train(recovery_time ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(2, -10, length = 200))),
                  trControl = ctrl1)
enet.fit$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar)

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
```

## Ridge regression: one SE rule

```{r}
ctrl_1se <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")

set.seed(11)
ridge.fit_1se <- train(recovery_time ~ . ,
                       data = training_data,
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 0,
                                              lambda = exp(seq(6, -6, length = 200))),
                       trControl = ctrl_1se)

plot(ridge.fit_1se, xTrans = log)

ridge.fit_1se$bestTune

coef(ridge.fit_1se$finalModel, ridge.fit_1se$bestTune$lambda)
```

## Lasso: one SE rule

```{r}
set.seed(11)
lasso.fit_1se <- train(recovery_time ~ .,
                       data = training_data,
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 1,
                                              lambda = exp(seq(6, -6, length = 200))),
                       trControl = ctrl_1se)
plot(lasso.fit_1se, xTrans = log)

lasso.fit_1se$bestTune

coef(lasso.fit_1se$finalModel, lasso.fit_1se$bestTune$lambda)
```

## Elastic net: one SE rule

```{r}
set.seed(11)
enet.fit_1se <- train(recovery_time ~ .,
                      data = training_data,
                      method = "glmnet",
                      tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(2, -10, length = 200))),
                      trControl = ctrl_1se)
enet.fit_1se$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit_1se, par.settings = myPar)

coef(enet.fit_1se$finalModel, enet.fit_1se$bestTune$lambda)
```

## PLS

```{r}
# training data
x <- model.matrix(recovery_time ~ ., training_data)[, -1]
y <- training_data$recovery_time

# test data
x2 <- model.matrix(recovery_time ~ .,testing_data)[, -1]
y2 <- testing_data$recovery_time

set.seed(11)
house_pls.fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:15),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))
predy2.pls2 <- predict(house_pls.fit, newdata = x2)
sqrt(mean((y2 - predy2.pls2)^2))

ggplot(house_pls.fit, highlight = TRUE)
house_pls.fit$finalModel$ncomp
```

## MARS

```{r}
mars_grid <- expand.grid(degree = 1:3, 
                         nprune = 2:15)

set.seed(11)
mars_fit <- train(x, y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

ggplot(mars_fit)

mars_fit$bestTune

coef(mars_fit$finalModel) 
```

## GAM

```{r}
set.seed(1)
gam_fit <- train(x, y,
                 method = "gam",
                 trControl = ctrl1)

gam_fit$bestTune
```

