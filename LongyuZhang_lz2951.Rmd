---
title: "P8106_midterm"
author: "lz2951"
date: "2024-03-28"
output: 
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{amssymb}
---

```{r}
library(tidyverse)
library(ggcorrplot)
library(pheatmap)
library(caret)
library(tidymodels)
```

# Import Data

```{r}
load("recovery.RData")

str(dat)

recovery = dat |>
  janitor::clean_names() |>
  mutate(gender = as.factor(gender), 
         hypertension = as.factor(hypertension),
         diabetes = as.factor(diabetes),
         vaccine = as.factor(vaccine),
         severity = as.factor(severity),
         study = as.factor(study))

str(recovery)
```

# Exploratory analysis and data visualization

```{r}
skimr::skim(recovery) |>
  select(-numeric.hist)
```

## Analysis between numeric predictors

```{r}
recovery_numeric = 
  recovery |>
  select(where(is.numeric), -id)

# recovery_numeric

ggcorrplot(cor(recovery_numeric), lab = T)

recovery_numeric_long = 
  recovery_numeric |>
  gather(key = "predictor", value = "value", -recovery_time)

# recovery_numeric_long

ggplot(recovery_numeric_long, aes(x = value, y = recovery_time)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~predictor, scales = "free")
```

## Analysis between factor predictors

```{r}
recovery_factor = 
  recovery |>
  select(where(is.factor), recovery_time)

# recovery_factor

recovery_factor_nonresp =
  recovery |>
  select(where(is.factor))

# recovery_factor_nonresp

chi_sq_matrix = matrix(NA, ncol = ncol(recovery_factor_nonresp), nrow = ncol(recovery_factor_nonresp))
for (i in 1:(ncol(recovery_factor_nonresp)-1)) {
  for (j in (i+1):ncol(recovery_factor_nonresp)) {
    cross_table = table(recovery_factor_nonresp[,i],
                        recovery_factor_nonresp[,j])
    chi_sq_matrix[i,j] = chisq.test(cross_table)$p.value
  }
}

rownames(chi_sq_matrix) = colnames(recovery_factor_nonresp)
colnames(chi_sq_matrix) = colnames(recovery_factor_nonresp)

# chi_sq_matrix

chi_sq_matrix = t(chi_sq_matrix)

# chi_sq_matrix

pheatmap(chi_sq_matrix,
         cluster_rows = FALSE, cluster_cols = FALSE,
         show_rownames = TRUE, show_colnames = TRUE,
         legend = TRUE, display_numbers = TRUE)

recovery_factor_long = 
  recovery_factor |>
  gather(key = "predictor", value = "value", -recovery_time)

# recovery_factor_long

ggplot(recovery_factor_long, aes(x = value, y = recovery_time)) +
  geom_violin() +
  facet_wrap(~predictor, scales = "free")
```

## Analysis between numeric and factor predictors

```{r}
anova_matrix = matrix(NA, ncol = ncol(recovery_factor_nonresp), nrow = ncol(recovery_numeric))
for (i in 1:(ncol(recovery_numeric))) {
  for (j in 1:ncol(recovery_factor_nonresp)) {
    cross_dat = data.frame(num = recovery_numeric[,i], 
                           fac = recovery_factor_nonresp[,j])
    anova_matrix[i,j] = summary(aov(num ~ fac, data = cross_dat))[[1]]$"Pr(>F)"[[1]]
  }
}

# anova_matrix

rownames(anova_matrix) = colnames(recovery_numeric)
colnames(anova_matrix) = colnames(recovery_factor_nonresp)

pheatmap(anova_matrix,
         cluster_rows = FALSE, cluster_cols = FALSE,
         show_rownames = TRUE, show_colnames = TRUE,
         legend = TRUE, display_numbers = TRUE)
```

# Model training

Split dataset into training and testing data.

```{r}
set.seed(11)
data_split <- initial_split(recovery, prop = 0.8)

training_data <- training(data_split)
testing_data <- testing(data_split)
```

## Ridge regression

```{r}
ctrl1 <- trainControl(method = "cv", number = 10)

set.seed(11)
ridge.fit <- train(recovery_time ~ . ,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(6, -6, length = 200))),
                   trControl = ctrl1)

plot(ridge.fit, xTrans = log)

ridge.fit$bestTune

coef(ridge.fit$finalModel, ridge.fit$bestTune$lambda)
```

## Lasso

```{r}
set.seed(11)
lasso.fit <- train(recovery_time ~ .,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(6, -6, length = 200))),
                   trControl = ctrl1)
plot(lasso.fit, xTrans = log)

lasso.fit$bestTune

coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)
```

## Elastic net

```{r}
set.seed(11)
enet.fit <- train(recovery_time ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(2, -8, length = 200))),
                  trControl = ctrl1)
enet.fit$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar)

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
```

