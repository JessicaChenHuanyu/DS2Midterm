---
title: "P8106_midterm"
author: "lz2951"
date: "2024-03-28"
output: 
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{amssymb}
---

```{r}
library(tidyverse)
library(ggcorrplot)
library(pheatmap)
library(caret)
library(tidymodels)
```

# Import Data

```{r}
load("recovery.RData")

str(dat)

recovery = dat |>
  janitor::clean_names() |>
  mutate(gender = as.factor(gender), 
         hypertension = as.factor(hypertension),
         diabetes = as.factor(diabetes),
         vaccine = as.factor(vaccine),
         severity = as.factor(severity),
         study = as.factor(study)) |>
  select(-id)

str(recovery)
```

# Exploratory analysis and data visualization

```{r}
skimr::skim(recovery) |>
  select(-numeric.hist)
```

## Analysis between numeric predictors

```{r}
recovery_numeric = 
  recovery |>
  select(where(is.numeric))

# recovery_numeric

ggcorrplot(cor(recovery_numeric), lab = T)

recovery_numeric_long = 
  recovery_numeric |>
  gather(key = "predictor", value = "value", -recovery_time)

# recovery_numeric_long

ggplot(recovery_numeric_long, aes(x = value, y = recovery_time)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~predictor, scales = "free")
```

## Analysis between factor predictors

```{r}
recovery_factor = 
  recovery |>
  select(where(is.factor), recovery_time)

# recovery_factor

recovery_factor_nonresp =
  recovery |>
  select(where(is.factor))

# recovery_factor_nonresp

chi_sq_matrix = matrix(NA, ncol = ncol(recovery_factor_nonresp), nrow = ncol(recovery_factor_nonresp))
for (i in 1:(ncol(recovery_factor_nonresp)-1)) {
  for (j in (i+1):ncol(recovery_factor_nonresp)) {
    cross_table = table(recovery_factor_nonresp[,i],
                        recovery_factor_nonresp[,j])
    chi_sq_matrix[i,j] = chisq.test(cross_table)$p.value
  }
}

rownames(chi_sq_matrix) = colnames(recovery_factor_nonresp)
colnames(chi_sq_matrix) = colnames(recovery_factor_nonresp)

# chi_sq_matrix

chi_sq_matrix = t(chi_sq_matrix)

# chi_sq_matrix

pheatmap(chi_sq_matrix,
         cluster_rows = FALSE, cluster_cols = FALSE,
         show_rownames = TRUE, show_colnames = TRUE,
         legend = TRUE, display_numbers = TRUE)

recovery_factor_long = 
  recovery_factor |>
  gather(key = "predictor", value = "value", -recovery_time)

# recovery_factor_long

ggplot(recovery_factor_long, aes(x = value, y = recovery_time)) +
  geom_violin() +
  facet_wrap(~predictor, scales = "free")
```

## Analysis between numeric and factor predictors

```{r}
anova_matrix = matrix(NA, ncol = ncol(recovery_factor_nonresp), nrow = ncol(recovery_numeric))
for (i in 1:(ncol(recovery_numeric))) {
  for (j in 1:ncol(recovery_factor_nonresp)) {
    cross_dat = data.frame(num = recovery_numeric[,i], 
                           fac = recovery_factor_nonresp[,j])
    anova_matrix[i,j] = summary(aov(num ~ fac, data = cross_dat))[[1]]$"Pr(>F)"[[1]]
  }
}

# anova_matrix

rownames(anova_matrix) = colnames(recovery_numeric)
colnames(anova_matrix) = colnames(recovery_factor_nonresp)

pheatmap(anova_matrix,
         cluster_rows = FALSE, cluster_cols = FALSE,
         show_rownames = TRUE, show_colnames = TRUE,
         legend = TRUE, display_numbers = TRUE)
```

# Model training

Split dataset into training and testing data.

```{r}
set.seed(11)
data_split <- initial_split(recovery, prop = 0.8)

training_data <- training(data_split)
testing_data <- testing(data_split)
```

## Ridge regression

```{r}
ctrl1 <- trainControl(method = "cv", number = 10)

set.seed(11)
ridge.fit <- train(recovery_time ~ . ,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(6, -6, length = 200))),
                   trControl = ctrl1)

plot(ridge.fit, xTrans = log)

ridge.fit$bestTune

coef(ridge.fit$finalModel, ridge.fit$bestTune$lambda)
```

## Lasso

```{r}
set.seed(11)
lasso.fit <- train(recovery_time ~ .,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(6, -6, length = 200))),
                   trControl = ctrl1)
plot(lasso.fit, xTrans = log)

lasso.fit$bestTune

coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)
```

## Elastic net

```{r}
set.seed(11)
enet.fit <- train(recovery_time ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(2, -10, length = 200))),
                  trControl = ctrl1)
enet.fit$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar)

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
```

## Ridge regression: one SE rule

```{r}
ctrl_1se <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")

set.seed(11)
ridge.fit_1se <- train(recovery_time ~ . ,
                       data = training_data,
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 0,
                                              lambda = exp(seq(6, -6, length = 200))),
                       trControl = ctrl_1se)

plot(ridge.fit_1se, xTrans = log)

ridge.fit_1se$bestTune

coef(ridge.fit_1se$finalModel, ridge.fit_1se$bestTune$lambda)
```

## Lasso: one SE rule

```{r}
set.seed(11)
lasso.fit_1se <- train(recovery_time ~ .,
                       data = training_data,
                       method = "glmnet",
                       tuneGrid = expand.grid(alpha = 1,
                                              lambda = exp(seq(6, -6, length = 200))),
                       trControl = ctrl_1se)
plot(lasso.fit_1se, xTrans = log)

lasso.fit_1se$bestTune

coef(lasso.fit_1se$finalModel, lasso.fit_1se$bestTune$lambda)
```

## Elastic net: one SE rule

```{r}
set.seed(11)
enet.fit_1se <- train(recovery_time ~ .,
                      data = training_data,
                      method = "glmnet",
                      tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(2, -10, length = 200))),
                      trControl = ctrl_1se)
enet.fit_1se$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.fit_1se, par.settings = myPar)

coef(enet.fit_1se$finalModel, enet.fit_1se$bestTune$lambda)
```

## PLS

```{r}
# training data
x <- model.matrix(recovery_time ~ ., training_data)[, -1]
y <- training_data$recovery_time

# test data
x2 <- model.matrix(recovery_time ~ .,testing_data)[, -1]
y2 <- testing_data$recovery_time

set.seed(11)
pls.fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:15),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

ggplot(pls.fit, highlight = TRUE)
pls.fit$finalModel$ncomp
```

## MARS

```{r}
mars_grid <- expand.grid(degree = 1:3, 
                         nprune = 2:15)

set.seed(11)
mars.fit <- train(x, y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

ggplot(mars.fit)

mars.fit$bestTune

coef(mars.fit$finalModel) 
```

## GAM

```{r}
set.seed(1)
gam.fit <- train(x, y,
                 method = "gam",
                 trControl = ctrl1)

gam.fit$bestTune
```

## Comparing different models

```{r}
set.seed(11)

resamp <- resamples(list(ridge = ridge.fit, enet = enet.fit, lasso = lasso.fit, pls = pls.fit,
                         ridge_1se = ridge.fit_1se, enet_1se = enet.fit_1se, lasso_1se = lasso.fit_1se,
                         mars = mars.fit, gam = gam.fit))

summary(resamp)

parallelplot(resamp, metric = "RMSE")

bwplot(resamp, metric = "RMSE")
```

# Results

```{r}
pred_ridge = predict(ridge.fit, newdata = testing_data)
rmse_ridge = sqrt(mean((testing_data$recovery_time - pred_ridge)^2))
rmse_ridge

pred_lasso = predict(lasso.fit, newdata = testing_data)
rmse_lasso = sqrt(mean((testing_data$recovery_time - pred_lasso)^2))
rmse_lasso

pred_enet = predict(enet.fit, newdata = testing_data)
rmse_enet = sqrt(mean((testing_data$recovery_time - pred_enet)^2))
rmse_enet

pred_ridge_1se = predict(ridge.fit_1se, newdata = testing_data)
rmse_ridge_1se = sqrt(mean((testing_data$recovery_time - pred_ridge_1se)^2))
rmse_ridge_1se

pred_lasso_1se = predict(lasso.fit_1se, newdata = testing_data)
rmse_lasso_1se = sqrt(mean((testing_data$recovery_time - pred_lasso_1se)^2))
rmse_lasso_1se

pred_enet_1se = predict(enet.fit_1se, newdata = testing_data)
rmse_enet_1se = sqrt(mean((testing_data$recovery_time - pred_enet_1se)^2))
rmse_enet_1se

pred_pls = predict(pls.fit, newdata = x2)
rmse_pls = sqrt(mean((y2 - pred_pls)^2))
rmse_pls

pred_mars = predict(mars.fit, new_data = x2)
rmse_mars = sqrt(mean((y2 - pred_mars)^2))
rmse_mars

pred_gam = predict(gam.fit, new_data = x2)
rmse_gam = sqrt(mean((y2 - pred_gam)^2))
rmse_gam

models = c("ridge_regression", "lasso", "elastic_net", "ridge_regression_1se", "lasso_1se",
           "elastic_net_1se", "pls", "mars", "gam")
rmse_on_testing = c(rmse_ridge, rmse_lasso, rmse_enet, rmse_ridge_1se, rmse_lasso_1se, 
                    rmse_enet_1se, rmse_pls, rmse_mars, rmse_gam)
knitr::kable(data.frame(models, rmse_on_testing))
```

